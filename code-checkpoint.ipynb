{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cda6ab63",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0c95189",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>sarcastic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The only thing I got from college is a caffein...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I love it when professors draw a big question ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Remember the hundred emails from companies whe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Today my pop-pop told me I was not “forced” to...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@VolphanCarol @littlewhitty @mysticalmanatee I...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3463</th>\n",
       "      <td>The population spike in Chicago in 9 months is...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3464</th>\n",
       "      <td>You'd think in the second to last English clas...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3465</th>\n",
       "      <td>I’m finally surfacing after a holiday to Scotl...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3466</th>\n",
       "      <td>Couldn't be prouder today. Well done to every ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3467</th>\n",
       "      <td>Overheard as my 13 year old games with a frien...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3468 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  tweet  sarcastic\n",
       "0     The only thing I got from college is a caffein...          1\n",
       "1     I love it when professors draw a big question ...          1\n",
       "2     Remember the hundred emails from companies whe...          1\n",
       "3     Today my pop-pop told me I was not “forced” to...          1\n",
       "4     @VolphanCarol @littlewhitty @mysticalmanatee I...          1\n",
       "...                                                 ...        ...\n",
       "3463  The population spike in Chicago in 9 months is...          0\n",
       "3464  You'd think in the second to last English clas...          0\n",
       "3465  I’m finally surfacing after a holiday to Scotl...          0\n",
       "3466  Couldn't be prouder today. Well done to every ...          0\n",
       "3467  Overheard as my 13 year old games with a frien...          0\n",
       "\n",
       "[3468 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Reading JSON file with lines=True\n",
    "df = pd.read_csv(\"Sarcasm.csv\")\n",
    "\n",
    "# Display the dataframe\n",
    "df = df[['tweet','sarcastic']]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45d27545",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14df9a61",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8ffae93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from contractions import fix  # To handle contractions like don't -> do not\n",
    "\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')  # lemmatizer के लिए ज़रूरी\n",
    "\n",
    "# Initialize stopwords and lemmatizer\n",
    "# stop_words = set(stopwords.words('english'))\n",
    "# lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# def clean_text_advanced(text):\n",
    "#     # Expand contractions\n",
    "#     text = fix(text)\n",
    "    \n",
    "#     # Remove URLs\n",
    "#     text = re.sub(r'http\\S+', '', text)\n",
    "    \n",
    "#     # Remove HTML tags\n",
    "#     text = re.sub(r'<.*?>', '', text)\n",
    "    \n",
    "#     # Convert to lowercase\n",
    "#     text = text.lower()\n",
    "    \n",
    "#     # Remove punctuation\n",
    "#     text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "#     # Tokenize the text\n",
    "#     tokens = word_tokenize(text)\n",
    "    \n",
    "#     # Remove stopwords and lemmatize\n",
    "#     cleaned_tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    \n",
    "#     # Join tokens back to a single string\n",
    "#     cleaned_text = ' '.join(cleaned_tokens)\n",
    "    \n",
    "#     # Remove excessive whitespaces\n",
    "#     cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n",
    "    \n",
    "#     return cleaned_text\n",
    "\n",
    "# df['tweet'] = df['tweet'].apply(clean_text_advanced)\n",
    "\n",
    "\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "def clean_text_advanced(text):\n",
    "    text = fix(text)\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    tokens = tokenizer.tokenize(text)  # <--- safer tokenizer\n",
    "\n",
    "    cleaned_tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    cleaned_text = ' '.join(cleaned_tokens)\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n",
    "    return cleaned_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3cc39d4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>sarcastic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The only thing I got from college is a caffein...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I love it when professors draw a big question ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Remember the hundred emails from companies whe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Today my pop-pop told me I was not “forced” to...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@VolphanCarol @littlewhitty @mysticalmanatee I...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3463</th>\n",
       "      <td>The population spike in Chicago in 9 months is...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3464</th>\n",
       "      <td>You'd think in the second to last English clas...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3465</th>\n",
       "      <td>I’m finally surfacing after a holiday to Scotl...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3466</th>\n",
       "      <td>Couldn't be prouder today. Well done to every ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3467</th>\n",
       "      <td>Overheard as my 13 year old games with a frien...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3467 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  tweet  sarcastic\n",
       "0     The only thing I got from college is a caffein...          1\n",
       "1     I love it when professors draw a big question ...          1\n",
       "2     Remember the hundred emails from companies whe...          1\n",
       "3     Today my pop-pop told me I was not “forced” to...          1\n",
       "4     @VolphanCarol @littlewhitty @mysticalmanatee I...          1\n",
       "...                                                 ...        ...\n",
       "3463  The population spike in Chicago in 9 months is...          0\n",
       "3464  You'd think in the second to last English clas...          0\n",
       "3465  I’m finally surfacing after a holiday to Scotl...          0\n",
       "3466  Couldn't be prouder today. Well done to every ...          0\n",
       "3467  Overheard as my 13 year old games with a frien...          0\n",
       "\n",
       "[3467 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cfc7be11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sarcastic\n",
       "0    2600\n",
       "1     867\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sarcastic'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf797a7",
   "metadata": {},
   "source": [
    "# Data Balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2e71f7de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Balanced Class Distribution:\n",
      " sarcastic\n",
      "1    2600\n",
      "0    2600\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "# Splitting the data into features and labels\n",
    "X = df['tweet'].values.reshape(-1, 1)  # Reshaping for the oversampler\n",
    "y = df['sarcastic']\n",
    "\n",
    "# Applying Random Oversampling\n",
    "oversampler = RandomOverSampler(random_state=42)\n",
    "X_balanced, y_balanced = oversampler.fit_resample(X, y)\n",
    "\n",
    "# Creating a balanced DataFrame\n",
    "df = pd.DataFrame({'tweet': X_balanced.flatten(), 'sarcastic': y_balanced})\n",
    "\n",
    "# Check the new class distribution\n",
    "print(\"\\nBalanced Class Distribution:\\n\", df['sarcastic'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c09a1a",
   "metadata": {},
   "source": [
    "# Vectorization and train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d2650a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF Vectorization\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "tfidf = TfidfVectorizer() \n",
    "X = tfidf.fit_transform(df['tweet'])\n",
    "\n",
    "#  Define target variable\n",
    "y = df['sarcastic']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#  Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "27ab7525",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler(with_mean=False)  # `with_mean=False` due to sparse matrix\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test  = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dee9ed2",
   "metadata": {},
   "source": [
    "# Model Building and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "84fd56b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 83.75%\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.74      0.81       493\n",
      "           1       0.80      0.93      0.86       547\n",
      "\n",
      "    accuracy                           0.84      1040\n",
      "   macro avg       0.85      0.83      0.83      1040\n",
      "weighted avg       0.85      0.84      0.84      1040\n",
      "\n",
      "Confusion Matrix:\n",
      " [[363 130]\n",
      " [ 39 508]]\n"
     ]
    }
   ],
   "source": [
    "# 3. Train SVM Model\n",
    "from sklearn.svm import SVC \n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "model = SVC(kernel='linear')  # Linear kernel is common for text classification\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 4. Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# 5. Evaluate the Model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Display results\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "738225d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 92.12%\n",
      "Classification Report (Random Forest):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.96      0.92       493\n",
      "           1       0.96      0.89      0.92       547\n",
      "\n",
      "    accuracy                           0.92      1040\n",
      "   macro avg       0.92      0.92      0.92      1040\n",
      "weighted avg       0.92      0.92      0.92      1040\n",
      "\n",
      "Confusion Matrix (Random Forest):\n",
      " [[473  20]\n",
      " [ 62 485]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_model = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "print(f\"Random Forest Accuracy: {accuracy_rf * 100:.2f}%\")\n",
    "print(\"Classification Report (Random Forest):\\n\", classification_report(y_test, y_pred_rf))\n",
    "print(\"Confusion Matrix (Random Forest):\\n\", confusion_matrix(y_test, y_pred_rf))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daccf9b9",
   "metadata": {},
   "source": [
    "# Sarcasm Detection System Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "513fc608",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_sarcasm(new_headline):\n",
    "    cleaned_headline = clean_text_advanced(new_headline)  \n",
    "    transformed_headline = tfidf.transform([cleaned_headline])\n",
    "    prediction = model.predict(transformed_headline)\n",
    "    \n",
    "    if prediction == 1:\n",
    "        return \"Sarcastic\"\n",
    "    else:\n",
    "        return \"Not Sarcastic\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3a3fd505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for comment: 'Oh great, another Monday! I just love waking up early after the weekend.' -> \n",
      " Not Sarcastic\n"
     ]
    }
   ],
   "source": [
    "# Example usage of detection system\n",
    "test_headline = \"Oh great, another Monday! I just love waking up early after the weekend.\"\n",
    "\n",
    "print(f\"Prediction for comment: '{test_headline}' -> \\n {detect_sarcasm(test_headline)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c405e1c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for comment: 'Oh, how thoughtful! I really needed someone to explain the obvious to me.' -> \n",
      " Not Sarcastic\n"
     ]
    }
   ],
   "source": [
    "# Example usage of detection system\n",
    "test_headline = \"Oh, how thoughtful! I really needed someone to explain the obvious to me.\"\n",
    "print(f\"Prediction for comment: '{test_headline}' -> \\n {detect_sarcasm(test_headline)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e2b11a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "03c16678",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(model,open(\"model1.pkl\",'wb'))\n",
    "pickle.dump(tfidf,open(\"tfidf1.pkl\",'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e024fad6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bb0b98-6542-4703-af0e-a5ffedbb7e02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51090fc7-d435-4d34-8d8c-e9abc5f51bef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ce33b3-a802-405f-914c-5ac88cd26914",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63967bd2-a1d3-4242-8116-6847d9f1bf10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c15aa7-d27d-4295-9130-82ede9af9979",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610971d9-c295-49e8-90c9-f6cc5584593a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
